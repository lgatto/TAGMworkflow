\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={A Bioconductor workflow for the Bayesian analysis of spatial proteomics},
            pdfauthor={Oliver M. Crook, Lisa M. Breckels, Kathryn S. Lilley, Paul D.W. Kirk, Laurent Gatto},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{{#1}}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{{#1}}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\ImportTok}[1]{{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{{#1}}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{{#1}}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{{#1}}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{{#1}}}}
\newcommand{\BuiltInTok}[1]{{#1}}
\newcommand{\ExtensionTok}[1]{{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{{#1}}}
\newcommand{\RegionMarkerTok}[1]{{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{{#1}}}}
\newcommand{\NormalTok}[1]{{#1}}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{A Bioconductor workflow for the Bayesian analysis of spatial proteomics}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Oliver M. Crook, Lisa M. Breckels, Kathryn S. Lilley, Paul D.W. Kirk,
Laurent Gatto}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{Cambridge Centre for Proteomics, Department of Biochemistry, University
of Cambridge, Cambridge, UK \nsplit MRC Biostatistics Unit, Cambridge
Institute for Public Health, Cambridge, UK \nsplit de Duve Institute,
UCLouvain, Avenue Hippocrate 75, 1200 Brussels, Belgium}


\begin{document}
\maketitle

\newcommand{\diag}{\mathop{\mathrm{diag}}}

\section*{Abstract}\label{abstract}
\addcontentsline{toc}{section}{Abstract}

Knowledge of the subcellular location of a protein gives valuable
insight into its function. The field of spatial proteomics has become
increasingly popular due to improved multiplexing capabilities in
high-throughput mass-spectrometry, which have made it possible to
systematically localise thousands of proteins per experiment. In
parallel with these experimental advances, improved methods for
analysing spatial proteomics data have also been developed. In this
workflow, we demonstrate using \texttt{pRoloc} for the Bayesian analysis
of spatial proteomics data. We detail the software infrastructure and
then provide step-by-step guidance of the analysis, including setting up
a pipeline, assessing convergence, and interpreting downstream results.
In several places we provide additional details on Bayesian analysis to
provide users with a holistic view of Bayesian analysis for spatial
proteomics data.

\section{Introduction}\label{introduction}

Determining the the spatial subcellular distribution of proteins enables
novel insight into protein function (Crook et al. 2018). Many proteins
function within a single location within the cell, however it is
estimated that up to half of the proteome is thought to reside in
mutiple locations, with some of these undergoing dynamic relocalisation
(Thul et al. 2017). These phenomena lead to variability and uncertainty
in robustly assigning proteins to a unique localisation. Functional
compartmentalisation of proteins allows the cell to control biomolecular
pathways and biochemical processes within the cell. Therefore, proteins
with multiple localisation may therefore have multiple functional roles
(Jeffery 2009). Machine learning algorithms that fail to quantify
uncertainty are unable to draw deeper insight into understanding cell
biology from mass-spectrometry (MS) based spatial proteomics
experiments. Hence, quantifying uncertainty allows us to make rigorous
assessments of protein subcellular localisation and multi-localisation.

For proteins to carry out their functional role they must be localised
to the correct subcellular compartment, ensuring the biochemical
conditions for desired molecular interactions are met (Gibson 2009).
Many pathologies, including cancer and obesity are characterised by
protein mis-localisations (Olkkonen and Ikonen 2006, Laurila and Vihinen
(2009), Luheshi, Crowther, and Dobson (2008), De Matteis and Luini
(2011), Cody, Iampietro, and LÃ©cuyer (2013), Kau, Way, and Silver
(2004), Rodriguez, Au, and Henderson (2004), Latorre et al. (2005), Shin
et al. (2013), Siljee et al. (2018)). High-throughput spatial proteomics
technologies have seen rapid improvement over the last decade and now a
single experiment can provide spatial information on thousands of
proteins at once (Dunkley et al. 2006, Foster et al. (2006),
Christoforou et al. (2016), Geladaki et al. (2019)). As a result of
these spatial proteomics technologies many biological systems have been
characterised (Dunkley et al. 2006, Tan et al. (2009), Hall et al.
(2009), Breckels et al. (2013), Christoforou et al. (2016), Thul et al.
(2017)). The popularity of such methods is now evident with many new
studies in recent years (Christoforou et al. 2016, Beltran, Mathias, and
Cristea (2016), Jadot et al. (2017), Itzhak et al. (2017), Mendes et al.
(2017), Hirst et al. (2018), Davies et al. (2018), Orre et al. (2019),
Nightingale et al. (2019)).

Bayesian approaches to machine learning and statistics can provide more
insight, by providing uncertainty quantification (Gelman et al. 1995).
In a parametric Bayesian setting, a parametric model is proposed, along
with a statement about our prior beliefs of the model parameters. Bayes'
theorem tells us how to update the prior distribution of the parameters
to obtain the posterior distribution of the parameters after observing
the data. It is the posterior distribution which quantifies the
uncertainty in the parameters. This contrasts from a maximum-likelihood
approach where we obtain only a point estimate of the parameters.

Adopting a Bayesian framework for data analysis, though of much interest
to experimentalists, can be challenging. Once we have specified a
probabilistic model, computational approaches are typically used to
obtain the posterior distribution upon observation of the data. These
algorithms can have parameters that require tuning and a variety of
settings, hindering their practical use by those not familiar with
Bayesian methodology. Even once the algorithms have been correctly
set-up, assessments of convergence and guidance on how to interpret the
results are often sparse. This workflow presents a Bayesian analysis of
spatial proteomics to elucidate the process for practitioners. Our
workflow also provides a template for others interested in designing
tools for the biological community which rely on Bayesian inference.

Our model for the data is the t-augmented Gaussian mixture (TAGM) model
proposed in (Crook et al. 2018). Crook et al. (2018) provide a detailed
description of the model, rigorous comparisons and testing on many
spatial proteomics datasets, including a case study in which a
hyperLOPIT experiment is performed on mouse pluripotent stem cells
(Christoforou et al. 2016; Mulvey et al. 2017). Revisiting these details
is not the purpose of this computational protocol; rather we present how
to correctly use the software and provide step-by-step guidance for
interpreting the results.

In brief, the TAGM model posits that each annotated sub-cellular niche
can be modelled using a Gaussian distribution. Thus the full complement
of proteins within the cell is captured as a mixture of Gaussians. The
highly dynamic nature of the cell means that many proteins are not well
captured by any of these multivariate Gaussian distributions, and thus
the model also includes an outlier component, which is mathematically
described as a multivariate student's t distribution. The heavy tails of
the t distribution allow it to better capture dispersed proteins.

There are two approaches to perform inference in the TAGM model. The
first, which we refer to as TAGM MAP, allows us to obtain \emph{maximum
a posteriori} estimates of posterior localisation probabilities; that
is, the modal posterior probability that a protein localises to that
class. This approach uses the expectation-maximisation (EM) algorithm to
perform inference (Dempster, Laird, and Rubin 1977). Whilst this is a
interpretable summary of the TAGM model, it only provides point
estimates. For a richer analysis, we also present a Markov-chain
Monte-Carlo (MCMC) method to perform fully Bayesian inference in our
model, allowing us to obtain full posterior localisation distributions.
This method is referred to as TAGM MCMC throughout the text.

This workflow begins with a brief review of some of the basic features
of mass spectrometry-based spatial proteomics data, including our
state-of-the-art computational infrastructure and bespoke software
suite. We then present each method in turn, detailing how to obtain high
quality results. We provide an extended discussion of the TAGM MCMC
method to highlight some of the challenges that may arise when applying
this method. This includes how to assess convergence of MCMC methods, as
well as methods for manipulating the output. We then take the processed
output and explain how to interpret the results, as well as providing
some tools for visualisation. We conclude with some remarks and
directions for the future.

\section{Getting started and
infrastructure}\label{getting-started-and-infrastructure}

In this workflow, we are using version 1.23.2 of \texttt{pRoloc} (Gatto
et al. 2014). The package \texttt{pRoloc} contains algorithms and
methods for analysing spatial proteomics data, building on the
\texttt{MSnSet} structure provided in \texttt{MSnbase}. The
\texttt{pRolocdata} package provides many annotated datasets from a
variety of species and experimental procedures. The following code
chunks install and load the suite of packages require for the analysis.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{if (!}\KeywordTok{require}\NormalTok{(}\StringTok{"BiocManager"}\NormalTok{))}
    \KeywordTok{install.package}\NormalTok{(}\StringTok{"BiocManager"}\NormalTok{)}
\NormalTok{BiocManager::}\KeywordTok{install}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"pRoloc"}\NormalTok{, }\StringTok{"pRolocdata"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"pRoloc"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## This is pRoloc version 1.23.2 
##   Visit https://lgatto.github.io/pRoloc/ to get started.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"pRolocdata"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## This is pRolocdata version 1.20.0.
## Use 'pRolocdata()' to list available data sets.
\end{verbatim}

We assume that we have a MS-based spatial proteomics dataset contained
in a \texttt{MSnSet} structure. For information on how to import data,
perform basic data processing, quality control, supervised machine
learning and transfer learning we refer the reader to (Breckels, Mulvey,
et al. 2016). Here, we start by loading a spatial proteomics dataset on
mouse E14TG2a embryonic stem cells (Breckels, Holden, et al. 2016). The
LOPIT protocol (Dunkley et al. 2004; Dunkley et al. 2006) was used and
the normalised intensity of proteins from eight iTRAQ 8-plex labelled
fraction are provided. The methods provided here are independent of
labelling procedure, fractionation process or workflow. Examples of
valid experimental protocols are LOPIT (Dunkley et al. 2004), hyperLOPIT
(Christoforou et al. 2016; Mulvey et al. 2017), label-free methods such
as PCP (Foster et al. 2006), and when fractionation is perform by
differential centrifugation (Itzhak et al. 2016; Geladaki et al. 2019).

In the code chunk below, we load the aforementioned dataset. The
printout demonstrates that this experiment quantified 2031 proteins over
8 fractions.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(}\StringTok{"E14TG2aR"}\NormalTok{) }\CommentTok{# load experimental data}
\NormalTok{E14TG2aR}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## MSnSet (storageMode: lockedEnvironment)
## assayData: 2031 features, 8 samples 
##   element names: exprs 
## protocolData: none
## phenoData
##   sampleNames: n113 n114 ... n121 (8 total)
##   varLabels: Fraction.information
##   varMetadata: labelDescription
## featureData
##   featureNames: Q62261 Q9JHU4 ... Q9EQ93 (2031 total)
##   fvarLabels: Uniprot.ID UniprotName ... markers (8 total)
##   fvarMetadata: labelDescription
## experimentData: use 'experimentData(object)'
## Annotation:  
## - - - Processing information - - -
## Loaded on Thu Jul 16 15:02:29 2015. 
## Normalised to sum of intensities. 
## Added markers from  'mrk' marker vector. Thu Jul 16 15:02:29 2015 
##  MSnbase version: 1.17.12
\end{verbatim}

In figure \ref{fig:e14pca1}, we can visualise the mouse stem cell
dataset use the \texttt{plot2D} function. We observe that some of the
organelle classes overlap and this is a typical feature of biological
datasets. Thus, it is vital to perform uncertainty quantification when
analysing biological data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot2D}\NormalTok{(E14TG2aR)}
\KeywordTok{addLegend}\NormalTok{(E14TG2aR, }\DataTypeTok{where =} \StringTok{"topleft"}\NormalTok{, }\DataTypeTok{cex =} \FloatTok{0.6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[htbp]
\centering
\includegraphics{TAGMworkflow_files/figure-latex/e14pca1-1.pdf}
\caption{\label{fig:e14pca1}First two principal components of mouse stem
cell data.}
\end{figure}

\newpage

\section{\texorpdfstring{Methods: \emph{TAGM
MAP}}{Methods: TAGM MAP}}\label{methods-tagm-map}

\subsection{Introduction to TAGM MAP}\label{introduction-to-tagm-map}

We can use \emph{maximum a posteriori} (MAP) estimation to perform
Bayesian parameter estimation for our model. The \emph{maximum a
posteriori} estimate is the mode of the posterior distribution and can
be used to provide a point estimate summary of the posterior
localisation probabilities. In contrast to TAGM MCMC (see later), it
does not provide samples from the posterior distribution, however it
allows for faster inference by using an extended version of the
expectation-maximisation (EM) algorithm. The EM algorithm iterates
between an expectation step and a maximisation step. This allows us to
find parameters which maximise the logarithm of the posterior, in the
presence of latent (unobserved) variables. The EM algorithm is
guaranteed to converge to a local mode. The code chunk below executes
the \texttt{tagmMapTrain} function for a default of 100 iterations. We
use the default priors for simplicity and convenience, however they can
be changed, which we explain in a later section. The output is an object
of class \texttt{MAPParams}, that captures the details of the TAGM MAP
model.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\NormalTok{mappars <-}\StringTok{ }\KeywordTok{tagmMapTrain}\NormalTok{(E14TG2aR)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## co-linearity detected; a small multiple of
##               the identity was added to the covariance
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mappars}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Object of class "MAPParams"
##  Method: MAP
\end{verbatim}

\subsubsection*{Aside: collinearity}\label{aside-collinearity}
\addcontentsline{toc}{subsubsection}{Aside: collinearity}

The previous code chunk outputs a message concerning data collinearity.
This is because the covariance matrix of the data has become
ill-conditioned and as a result the inversion of this matrix becomes
unstable with floating point arithmetic. This can lead to the failure of
standard matrix algorithms upon which our method depends. In this case,
it is standard practice to add a small multiple of the identity to
stabilise this matrix. The printed message is a statement that this
operation has been performed for these data.

\subsection{Model visualisation}\label{model-visualisation}

The results of the modelling can be visualised with the
\texttt{plotEllipse} function on figure \ref{fig:e14ellipse}. The outer
ellipse contains 99\% of the total probability whilst the middle and
inner ellipses contain 95\% and 90\% of the probability respectively.
The centres of the clusters are represented by black circumpunct
(circled dot). We can also plot the model in other principal components.
The code chunk below plots the probability ellipses along the first and
second, as well as the fourth principal component. The user can change
the components visualised by altering the \texttt{dims} argument.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\KeywordTok{plotEllipse}\NormalTok{(E14TG2aR, mappars)}
\KeywordTok{plotEllipse}\NormalTok{(E14TG2aR, mappars, }\DataTypeTok{dims =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[htbp]
\centering
\includegraphics{TAGMworkflow_files/figure-latex/e14ellipse-1.pdf}
\caption{\label{fig:e14ellipse}PCA plot with probability ellipses along PC 1
and 2 (left) and PC 1 and 4 (right).}
\end{figure}

\subsection{The expectation-maximisation
algorithm}\label{the-expectation-maximisation-algorithm}

The EM algorithm is iterative; that is, the algorithm iterates between
an expectation step and a maximisation step until the value of the
log-posterior does not change (Dempster, Laird, and Rubin 1977). This
fact can be used to assess the convergence of the EM algorithm. The
value of the log-posterior at each iteration can be accessed with the
\texttt{logPosteriors} function on the \texttt{MAPParams} object. The
code chuck below plots the log posterior at each iteration and we see on
figure \ref{fig:mapconverge} the algorithm rapidly plateaus and so we
have achieved convergence. If convergence has not been reached during
this time, we suggest to increase the number of iterations by changing
the parameter \texttt{numIter} in the \texttt{tagmMapTrain} method. In
practice, it is not unexpected to observe small fluctuations due to
numerical errors and this should not concern users.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}\KeywordTok{logPosteriors}\NormalTok{(mappars), }\DataTypeTok{type =} \StringTok{"b"}\NormalTok{, }\DataTypeTok{col =} \StringTok{"blue"}\NormalTok{,}
     \DataTypeTok{cex =} \FloatTok{0.3}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"log-posterior"}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{"iteration"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[htbp]
\centering
\includegraphics{TAGMworkflow_files/figure-latex/mapconverge-1.pdf}
\caption{\label{fig:mapconverge}Log-posterior at each iteration of the EM
algorithm demonstrating convergence.}
\end{figure}

The code chuck below uses the \texttt{mappars} object generated above,
along with the \texttt{E14RG2aR} dataset, to classify the proteins of
unknown localisation using \texttt{tagmPredict} function. The results of
running \texttt{tagmPredict} are appended to the \texttt{fData} columns
of the \texttt{MSnSet}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{E14TG2aR <-}\StringTok{ }\KeywordTok{tagmPredict}\NormalTok{(E14TG2aR, mappars) }\CommentTok{# Predict protein localisation}
\end{Highlighting}
\end{Shaded}

The new feature variables that are generated are:

\begin{itemize}
\tightlist
\item
  \texttt{tagm.map.allocation}: the TAGM MAP predictions for the most
  probable protein sub-cellular allocation.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(}\KeywordTok{fData}\NormalTok{(E14TG2aR)$tagm.map.allocation)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##          40S Ribosome          60S Ribosome               Cytosol 
##                    34                    85                   328 
## Endoplasmic reticulum              Lysosome         Mitochondrion 
##                   284                   147                   341 
##   Nucleus - Chromatin   Nucleus - Nucleolus       Plasma membrane 
##                   143                   322                   326 
##            Proteasome 
##                    21
\end{verbatim}

\begin{itemize}
\tightlist
\item
  \texttt{tagm.map.probability}: the posterior probability for the
  protein sub-cellular allocations.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(}\KeywordTok{fData}\NormalTok{(E14TG2aR)$tagm.map.probability)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## 0.00000 0.06963 0.93943 0.63829 0.99934 1.00000
\end{verbatim}

\begin{itemize}
\tightlist
\item
  \texttt{tagm.map.outlier}: the posterior probability for that protein
  to belong to the outlier component rather than any annotated
  component.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(}\KeywordTok{fData}\NormalTok{(E14TG2aR)$tagm.map.outlier)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
## 0.0000000 0.0002363 0.0305487 0.3452624 0.9249810 1.0000000
\end{verbatim}

We can visualise the results by scaling the pointer according the
posterior localisation probabilities. To do this we extract the MAP
localisation probabilities from the feature columns of the the
\texttt{MSnSet} and pass these to the \texttt{plot2D} function (figure
\ref{fig:mappca}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ptsze <-}\StringTok{ }\KeywordTok{fData}\NormalTok{(E14TG2aR)$tagm.map.probability }\CommentTok{# Scale pointer size}
\KeywordTok{plot2D}\NormalTok{(E14TG2aR, }\DataTypeTok{fcol =} \StringTok{"tagm.map.allocation"}\NormalTok{, }\DataTypeTok{cex =} \NormalTok{ptsze)}
\KeywordTok{addLegend}\NormalTok{(E14TG2aR, }\DataTypeTok{where =} \StringTok{"topleft"}\NormalTok{, }\DataTypeTok{cex =} \FloatTok{0.6}\NormalTok{, }\DataTypeTok{fcol =} \StringTok{"tagm.map.allocation"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[htbp]
\centering
\includegraphics{TAGMworkflow_files/figure-latex/mappca-1.pdf}
\caption{\label{fig:mappca}TAGM MAP allocations, where the pointer is scaled
according to the localisation probability and coloured according to the
most probable subcellular niche.}
\end{figure}

The TAGM MAP method is easy to use and it is simple to check
convergence, however it is limited in that it can only provide point
estimates of the posterior localisation distributions. To obtain the
full posterior distributions and therefore a rich analysis of the data,
we use Markov-Chain Monte-Carlo methods. In our particular case, we use
a \emph{collapsed Gibbs sampler} (Smith and Roberts 1993).

\section{\texorpdfstring{Methods: \emph{TAGM MCMC} a brief
overview}{Methods: TAGM MCMC a brief overview}}\label{methods-tagm-mcmc-a-brief-overview}

The TAGM MCMC method allows a fully Bayesian analysis of spatial
proteomics datasets. It employs a collapsed Gibbs sampler to sample from
the posterior distribution of localisation probablities, providing a
rich analysis of the data. This section demonstrates the advantage of
taking a Bayesian approach and the biological information that can be
extracted from this analysis.

For those unfamiliar with Bayesian methodology, some of the key ideas
for a more complete understanding are as follows. Firstly, MCMC based
inference contrasts with MAP based inference in that it \textit{samples}
from the posterior distribution of localisation probabilities. Hence, we
do not just have a single estimate for each quantity but a distribution
of estimates. MCMC methods are a large class of algorithms used to
sample from a probability distribution, in our case the posterior
distribution of the parameters (Gilks, Richardson, and Spiegelhalter
1995). Once we have sampled from the posterior distribution, we can
estimate the mean of the posterior distribution by simply taking the
mean of the samples. In a similar fashion, we can obtain estimates of
other summaries of the posterior distribution.

A schematic of MCMC sampling is provided in figure \ref{fig:mcmcCartoon}
to aid understanding. Proteins, coloured blue, are visualised along two
variables of the data. Probability ellipses representing contours of a
probability distribution matching the distribution of the proteins are
overlaid. We now wish to obtain samples from this distribution. The MCMC
algorithm is initialised with a starting location, then at each
iteration a new value is proposed. These proposed values are either
accepted or rejected (according to a carefully computed acceptance
probability) and over many iterations the algorithm converges and
produces samples from the desired distribution. Samples from the mean of
this distribution are coloured in red in the schematic figure. A large
portion of the earlier samples may not reflect the true distribution,
because the MCMC sampler has yet to converge. These early samples are
usually discarded and this is referred to as burn-in. The next state of
the algorithm depends on its current state and this leads to
auto-correlation in the samples. To suppress this auto-correlation, we
only retain every \(r^{th}\) sample. This is known as thinning. The
details of burn-in and thinning are further explained in later sections.

\begin{figure}[htbp]
\centering
\includegraphics{TAGMworkflow_files/figure-latex/mcmcCartoon-1.pdf}
\caption{\label{fig:mcmcCartoon}A schematic figure of MCMC sampling.
Proteins are coloured in blue and probability ellipses are overlaid
representing contours of a probability distribution matching the
distribution of the proteins. MCMC samples from the mean of this
distribution are then coloured in red.}
\end{figure}

The TAGM MCMC method is computationally intensive and requires at least
modest processing power. Leaving the MCMC algorithm to run overnight on
a modern desktop is usually sufficient, however this, of course, depends
on the particular dataset being analysed. For guidance: it should not be
expected that the analysis will finish in just a couple of hours on a
medium specification laptop, for example.

To demonstrate the class structure and expected outputs of the TAGM MCMC
method, we run a brief analysis on a subset (400 randomly chosen
proteins) of the \texttt{tan2009r1} dataset from the
\texttt{pRolocdata}, purely for illustration. This is to provide a bare
bones analysis of these data without being held back by computational
requirements. We perform a complete demonstration and provide precise
details of the analysis of the stem cell dataset considered above in the
next section.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\KeywordTok{data}\NormalTok{(tan2009r1)}
\NormalTok{tan2009r1 <-}\StringTok{ }\NormalTok{tan2009r1[}\KeywordTok{sample}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(tan2009r1), }\DecValTok{400}\NormalTok{), ]}
\end{Highlighting}
\end{Shaded}

The first step is to run a few MCMC chains (below we use only 2 chains)
for a few iterations (we specify 3 iterations in the below code, but
typically we would suggest in the order of tens of thousands; see for
example the algorithms default settings by typing
\texttt{?tagmMcmcTrain}) using the \texttt{tagmMcmcTrain} function. This
function will generate a object of class \texttt{MCMCParams}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p <-}\StringTok{ }\KeywordTok{tagmMcmcTrain}\NormalTok{(}\DataTypeTok{object =} \NormalTok{tan2009r1, }\DataTypeTok{numIter =} \DecValTok{3}\NormalTok{,}
                   \DataTypeTok{burnin =} \DecValTok{1}\NormalTok{, }\DataTypeTok{thin =} \DecValTok{1}\NormalTok{, }\DataTypeTok{numChains =} \DecValTok{2}\NormalTok{)}
\NormalTok{p}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Object of class "MCMCParams"
## Method: TAGM.MCMC 
## Number of chains: 2
\end{verbatim}

Information for each MCMC chain is contained within the chains slot. If
needed, this information can be accessed manually. The function
\texttt{tagmMcmcProcess} processes the \texttt{MCMCParams} object and
populates the summary slot.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p <-}\StringTok{ }\KeywordTok{tagmMcmcProcess}\NormalTok{(p)}
\NormalTok{p}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Object of class "MCMCParams"
## Method: TAGM.MCMC 
## Number of chains: 2 
## Summary available
\end{verbatim}

The summary slot has now been populated to include basic summaries of
the MCMC chains, such as organelle allocations and localisation
probabilities. Protein information can be appended to the feature
columns of the \texttt{MSnSet} by using the \texttt{tagmPredict}
function, which extracts the required information from the summary slot
of the \texttt{MCMCParams} object.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res <-}\StringTok{ }\KeywordTok{tagmPredict}\NormalTok{(}\DataTypeTok{object =} \NormalTok{tan2009r1, }\DataTypeTok{params =} \NormalTok{p)}
\end{Highlighting}
\end{Shaded}

We can now access new variables:

\begin{itemize}
\tightlist
\item
  \texttt{tagm.mcmc.allocation}: the TAGM MCMC prediction for the most
  likely protein sub-cellular annotation.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(}\KeywordTok{fData}\NormalTok{(res)$tagm.mcmc.allocation)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Cytoskeleton            ER         Golgi      Lysosome mitochondrion 
##            12            98            20            10            39 
##       Nucleus    Peroxisome            PM    Proteasome  Ribosome 40S 
##            24             3            99            33            32 
##  Ribosome 60S 
##            30
\end{verbatim}

\begin{itemize}
\tightlist
\item
  \texttt{tagm.mcmc.probability}: the mean posterior probability for the
  protein sub-cellular allocations.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(}\KeywordTok{fData}\NormalTok{(res)$tagm.mcmc.probability)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.3273  0.8922  0.9894  0.9094  1.0000  1.0000
\end{verbatim}

We can also access other useful summaries of the MCMC methods:

\begin{itemize}
\item
  \texttt{tagm.mcmc.outlier} the posterior probability for the protein
  to belong to the outlier component.
\item
  \texttt{tagm.mcmc.probability.lowerquantile} and
  \texttt{tagm.mcmc.probability.upperquantile} are the lower and upper
  boundaries to the equi-tailed 95\% credible interval of
  \texttt{tagm.mcmc.probability}.
\item
  \texttt{tagm.mcmc.mean.shannon} a Monte-Carlo averaged Shannon
  entropy, which is a measure of uncertainty in the allocations.
\end{itemize}

\section{\texorpdfstring{Methods: \emph{TAGM MCMC} the
details}{Methods: TAGM MCMC the details}}\label{methods-tagm-mcmc-the-details}

This section explains how to manually manipulate the MCMC output of the
TAGM model. In the code chunk below, we load a pre-computed TAGM MCMC
model. The data file \texttt{e14tagm.rda} is available online\footnote{\url{https://drive.google.com/open?id=1zozntDhE6YZ-q8wjtQ-lxZ66EEszOGYi}}
and is not directly loaded into this package due to its size. The file
itself if around 500mb, which is too large to directly load into a
package.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{load}\NormalTok{(}\StringTok{"e14Tagm.rda"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The following code, which is not evaluated dynamically, was used to
produce the \texttt{tagmE14} \texttt{MCMCParams} object. We run the MCMC
algorithm for 20,000 iterations with 10,000 iterations discarded for
burn-in. We then thin the chain by 20. We ran 6 chains in parallel and
so we obtain 500 samples for each of the 6 chains, totalling 3,000
samples. The resulting file is assumed to be in our working directory.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{e14Tagm <-}\StringTok{ }\KeywordTok{tagmMcmcTrain}\NormalTok{(E14TG2aR,}
                         \DataTypeTok{numIter =} \DecValTok{20000}\NormalTok{,}
                         \DataTypeTok{burnin =} \DecValTok{10000}\NormalTok{,}
                         \DataTypeTok{thin =} \DecValTok{20}\NormalTok{,}
                         \DataTypeTok{numChains =} \DecValTok{6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Manually inspecting the object, we see that it is a \texttt{MCMCParams}
object with 6 chains.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{e14Tagm}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Object of class "MCMCParams"
## Method: TAGM.MCMC 
## Number of chains: 6
\end{verbatim}

\subsection{Data exploration and convergence
diagnostics}\label{data-exploration-and-convergence-diagnostics}

Assessing whether or not an MCMC algorithm has converged is challenging.
Assessing and diagnosing convergence is an active area of research and
throughout the 1990s many approaches were proposed (Geweke 1992; Gelman
and Rubin 1992; Roberts and Smith 1994; Brooks and Gelman 1998). We
provide a more detailed exploration of this issue, but readers should
bare in mind that the methods provided below are diagnostics and cannot
guarantee convergence. We direct readers to several important works in
the literature discussing the assessment of convergence. Users that do
not assess convergence and base their downstream analysis on unconverged
chains are likely to obtain poor quality results.

We first assess convergence using a parallel chains approach. We find
producing multiple chains is benificial not only for computational
advantages but also for analysis of convergence of our chains.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{## Get number of chains}
\NormalTok{nChains <-}\StringTok{ }\KeywordTok{length}\NormalTok{(e14Tagm)}
\NormalTok{nChains}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 6
\end{verbatim}

The following code chunks set up a manual convergence diagnostic check.
We make use of objects and methods in the package
\emph{\href{https://CRAN.R-project.org/package=coda}{coda}} to perform
this analysis (Plummer et al. 2006). Our function below automatically
coerces our objects into
\emph{\href{https://CRAN.R-project.org/package=coda}{coda}} for ease of
analysis. We first calculate the total number of outliers at each
iteration of each chain and, if the algorithm has converged, this number
should be the same (or very similar) across all 6 chains.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{## Convergence diagnostic to see if we need to discard any}
\NormalTok{## iterations or entire chains: compute the number of outliers for}
\NormalTok{## each iteration for each chain}
\NormalTok{out <-}\StringTok{ }\KeywordTok{mcmc_get_outliers}\NormalTok{(e14Tagm)}
\end{Highlighting}
\end{Shaded}

We can observe this from the trace plots and histograms for each MCMC
chain (figure \ref{fig:mcmctraceHidden}). Unconverged chains should be
discarded from downstream analysis.

\begin{figure}[htbp]
\centering
\includegraphics{TAGMworkflow_files/figure-latex/mcmctraceHidden-1.pdf}
\caption{\label{fig:mcmctraceHidden}Trace (left) and density (right) of the
6 MCMC chains.}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{## Using coda S3 objects to produce trace plots and histograms}
\NormalTok{for (i in }\KeywordTok{seq_len}\NormalTok{(nChains))}
    \KeywordTok{plot}\NormalTok{(out[[i]], }\DataTypeTok{main =} \KeywordTok{paste}\NormalTok{(}\StringTok{"Chain"}\NormalTok{, i), }\DataTypeTok{auto.layout =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{col =} \NormalTok{i)}
\end{Highlighting}
\end{Shaded}

Chains 3, 5 and 6 are centred around an average of 153, with rapid back
and forth oscillations. Chain 2 should be immediately discarded, since
it has a large jump in the chain with clearly skewed histogram. The
other two chains oscillate differently with contrasting quantiles to the
3 chains (3, 5 and 6) that agree with one another, suggesting these
chains have yet to converge. We can use the
\emph{\href{https://CRAN.R-project.org/package=coda}{coda}} package to
produce summaries of our chains. Here is the \texttt{coda} summary for
the third chain.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{## Chains average around 153 outliers}
\KeywordTok{summary}\NormalTok{(out[[}\DecValTok{3}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Iterations = 1:500
## Thinning interval = 1 
## Number of chains = 1 
## Sample size per chain = 500 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##           Mean             SD       Naive SE Time-series SE 
##       153.4520        14.0771         0.6295         0.6820 
## 
## 2. Quantiles for each variable:
## 
##  2.5%   25%   50%   75% 97.5% 
##   127   144   153   162   183
\end{verbatim}

\subsubsection{Applying the Gelman
diagnostic}\label{applying-the-gelman-diagnostic}

So far, our analysis appears promising. Three of our chains are centred
around an average of 153 outliers and there is no observed monotonicity
in our output. However, for a more rigorous and unbiased analysis of
convergence we can calculate the Gelman diagnostic using the
\emph{\href{https://CRAN.R-project.org/package=coda}{coda}} package
(Gelman and Rubin 1992; Brooks and Gelman 1998). This statistic is often
referred to as \(\hat{R}\) or the potential scale reduction factor. The
idea of the Gelman diagnostics is to compare the inter and intra chain
variances. The ratio of these quantities should be close to one. A more
detailed and in depth discussion can be found in the references. The
\emph{\href{https://CRAN.R-project.org/package=coda}{coda}} package also
reports the \(95\%\) upper confidence interval of the \(\hat{R}\)
statistic. In this case, our samples are approximately normally
distributed (see histograms on the right in figure
\ref{fig:mcmctraceHidden}). The
\emph{\href{https://CRAN.R-project.org/package=coda}{coda}} package
allows for transformations to improve normality of the data, and in some
cases we set the \texttt{transform} argument to apply log
transformation. Gelman and Rubin (1992) suggest that chains with
\(\hat{R}\) value of less than 1.2 are likely to have converged.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{gelman.diag}\NormalTok{(out, }\DataTypeTok{transform =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Potential scale reduction factors:
## 
##      Point est. Upper C.I.
## [1,]       1.14       1.32
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{gelman.diag}\NormalTok{(out[}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{6}\NormalTok{)], }\DataTypeTok{transform =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Potential scale reduction factors:
## 
##      Point est. Upper C.I.
## [1,]       1.13       1.31
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{gelman.diag}\NormalTok{(out[}\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{6}\NormalTok{)], }\DataTypeTok{transform =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Potential scale reduction factors:
## 
##      Point est. Upper C.I.
## [1,]          1       1.01
\end{verbatim}

In all cases, we see that the Gelman diagnostic for convergence is
\textless{} 1.2. However, the upper confidence interval is 1.32 when all
chains are used; 1.31 when chain 2 is removed and when chains 1, 2 and 4
are removed the upper confidence interval is 1.01 indicating that the
MCMC algorithm for chains 3,5 and 6 might have converged.

We can also look at the Gelman diagnostics statistics for groups or
pairs of chains. The first line below computes the Gelman diagnostic
across the first three chains, whereas the second calculates the
diagnostic between chain 3 and chain 5.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{gelman.diag}\NormalTok{(out[}\DecValTok{1}\NormalTok{:}\DecValTok{3}\NormalTok{], }\DataTypeTok{transform =} \OtherTok{FALSE}\NormalTok{) }\CommentTok{# the upper C.I is 1.62}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Potential scale reduction factors:
## 
##      Point est. Upper C.I.
## [1,]       1.22       1.62
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{gelman.diag}\NormalTok{(out[}\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{5}\NormalTok{)], }\DataTypeTok{transform =} \OtherTok{TRUE}\NormalTok{) }\CommentTok{# the upper C.I is 1.01}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Potential scale reduction factors:
## 
##      Point est. Upper C.I.
## [1,]       1.01       1.01
\end{verbatim}

To assess another summary statistic, we can look at the mean component
allocation at each iteration of the MCMC algorithm and as before we
produce trace plots of this quantity (figure
\ref{fig:mcmctrace2hidden}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{meanAlloc <-}\StringTok{ }\KeywordTok{mcmc_get_meanComponent}\NormalTok{(e14Tagm)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[htbp]
\centering
\includegraphics{TAGMworkflow_files/figure-latex/mcmctrace2hidden-1.pdf}
\caption{\label{fig:mcmctrace2hidden}Trace (left) and density (right) of the
mean component allocation of the 6 MCMC chains.}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{for (i in }\KeywordTok{seq_len}\NormalTok{(nChains))}
    \KeywordTok{plot}\NormalTok{(meanAlloc[[i]], }\DataTypeTok{main =} \KeywordTok{paste}\NormalTok{(}\StringTok{"Chain"}\NormalTok{, i), }\DataTypeTok{auto.layout =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{col =} \NormalTok{i)}
\end{Highlighting}
\end{Shaded}

As before we can produce summaries of the data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(meanAlloc[[}\DecValTok{1}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Iterations = 1:500
## Thinning interval = 1 
## Number of chains = 1 
## Sample size per chain = 500 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##           Mean             SD       Naive SE Time-series SE 
##       5.686713       0.059112       0.002644       0.002644 
## 
## 2. Quantiles for each variable:
## 
##  2.5%   25%   50%   75% 97.5% 
## 5.552 5.646 5.692 5.728 5.795
\end{verbatim}

We can already observe that there are some slight difference between
these chains which raises suspicion that some of the chains may not have
converged. For example each chain appears to be centred around 5.7, but
chains 2 and 4 have clear jumps in the their trace plots. For a more
quantitative analysis, we again apply the Gelman diagnostics to these
summaries.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{gelman.diag}\NormalTok{(meanAlloc)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Potential scale reduction factors:
## 
##      Point est. Upper C.I.
## [1,]          1       1.01
\end{verbatim}

The above values are close to 1 and so we there are no significant
difference between the chains. As observed previously, chains 2 and 4
look quite different from the other chains and so we recalculate the
diagnostic excluding these chains. The computed Gelman diagnostic below
suggest that chains 3, 5 and 6 have converged and that we should discard
chains 1, 2 and 4 from further analysis.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{gelman.diag}\NormalTok{(meanAlloc[}\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{6}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Potential scale reduction factors:
## 
##      Point est. Upper C.I.
## [1,]          1          1
\end{verbatim}

For a further check, we can look at the mean outlier probability at each
iteration of the MCMC algorithm and again computing the Gelman
diagnostics between chains 4, 5 and 6. An \(\hat{R}\) statistics of 1 is
indicative of convergence, since it is less than the recommend value of
1.2.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{meanoutProb <-}\StringTok{ }\KeywordTok{mcmc_get_meanoutliersProb}\NormalTok{(e14Tagm)}
\KeywordTok{gelman.diag}\NormalTok{(meanoutProb[}\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Potential scale reduction factors:
## 
##      Point est. Upper C.I.
## [1,]          1       1.01
\end{verbatim}

\subsubsection{Applying the Geweke
diagnostic}\label{applying-the-geweke-diagnostic}

Along with the Gelman diagnostic, which uses parallel chains, we can
also apply a single chain analysis using the Geweke diagnostic (Geweke
1992). The Geweke diagnostic tests to see whether the mean calculated
from the first \(10\%\) of iterations is significantly different from
the mean calculated from the last \(50\%\) of iterations. If they are
significantly different, at say a level 0.01, then this is evidence that
particular chains have not converged. The following code chunk
calculates the Geweke diagnostic for each chain on the summarising
quantities we have previously computed.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{geweke_test}\NormalTok{(out)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           chain 1      chain 2  chain 3    chain 4    chain 5    chain 6
## z.value 0.5749775 8.816632e+00 0.470203 -0.3204500 -0.6270787 -0.7328168
## p.value 0.5653065 1.179541e-18 0.638210  0.7486272  0.5306076  0.4636702
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{geweke_test}\NormalTok{(meanAlloc)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           chain 1       chain 2    chain 3    chain 4   chain 5    chain 6
## z.value 1.1952967 -3.3737051063 -1.2232102 2.48951993 0.3605882 -0.1358850
## p.value 0.2319711  0.0007416377  0.2212503 0.01279157 0.7184073  0.8919122
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{geweke_test}\NormalTok{(meanoutProb)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           chain 1      chain 2   chain 3    chain 4    chain 5     chain 6
## z.value 0.1785882 1.205500e+01 0.6189637 -0.5164987 -0.2141086 -0.02379004
## p.value 0.8582611 1.825379e-33 0.5359403  0.6055062  0.8304624  0.98102008
\end{verbatim}

The first test suggests chain 2 has not converged, since the p-value is
less than \(10^{-10}\) suggesting that the mean in the first \(10\%\) of
iterations is significantly different from those in the final \(50\%\).
Moreover, the second test and third tests also suggest that chain 2 has
not converged. Furthermore, for the second test chain 4 has a marginally
small p-value, providing further evidence that this chain is of low
quality. These convergence diagnostics are not limited to the quantities
we have computed here and further diagnostics can be performed on any
summary of the data.

An important question to consider is whether removing an early portion
of the chain might lead to an improvement of the convergence
diagnostics. This might be particularly relevant if a chain converges
some iterations after our orginally specified \texttt{burn-in}. For
example, let us take the second Geweke test above, which suggested
chains 2 and 4 had not converged and see if discarding the initial
\(10\%\) of the chain improves the statistic. The function below removes
\(50\) samples, known as \texttt{burn-in}, from the beginning of each
chain and the output shows that we now have \(450\) samples in each
chain. In practice, as \(2\) chains are sufficient for good posterior
estimates and convergence we could simply discard chains \(2\) and \(4\)
and proceed with downstream analysis with the remaining chains.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{burne14Tagm <-}\StringTok{ }\KeywordTok{mcmc_burn_chains}\NormalTok{(e14Tagm, }\DecValTok{50}\NormalTok{)}
\KeywordTok{chains}\NormalTok{(burne14Tagm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Object of class "MCMCChains"
##  Number of chains: 6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{chains}\NormalTok{(burne14Tagm)[[}\DecValTok{4}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Object of class "MCMCChain"
##  Number of components: 10 
##  Number of proteins: 1663 
##  Number of iterations: 450
\end{verbatim}

The following function recomputes the number of outliers in each chain
at each iteration of each Markov-chain.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{newout <-}\StringTok{ }\KeywordTok{mcmc_get_outliers}\NormalTok{(burne14Tagm)}
\end{Highlighting}
\end{Shaded}

The code chuck below computes the Geweke diagnostic for this new
truncated chain and demonstrates that chain 4 has an improved Geweke
diagnostic, whilst chain 2 does not. Thus, in practice, it maybe useful
to remove iterations from the beginning of the chain. However, as chain
4 did not pass the Gelman diagnostics we still discard it from
downstream analysis.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{geweke_test}\NormalTok{(newout)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            chain 1      chain 2    chain 3   chain 4   chain 5   chain 6
## z.value -0.1455345 6.379618e+00 -1.6392215 0.3836940 0.1241201 0.6654703
## p.value  0.8842889 1.775298e-10  0.1011671 0.7012053 0.9012202 0.5057497
\end{verbatim}

\subsection{Processing converged
chains}\label{processing-converged-chains}

Having made an assessment of convergence, we decide to discard chains
\(1,2\) and \(4\) from any further analysis. The code chunk below
removes these chains and creates a new object to store the converged
chains.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{removeChain <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{) }\CommentTok{# The chains to be removed}
\NormalTok{e14Tagm_converged <-}\StringTok{ }\NormalTok{e14Tagm[-removeChain] }\CommentTok{# Create new object}
\end{Highlighting}
\end{Shaded}

The \texttt{MCMCParams} object can be large and therefore if we have a
large number of samples we may want to subsample our chain, known as
\emph{thinning}, to reduce the number of samples. Thinning also has
another purpose. We may desire independent samples from our posterior
distribution but the MCMC algorithm produces auto-correlated samples.
Thinning can be applied to reduce the auto-correlation between samples.
The code chuck below, which is not evaluated, demonstrates retaining
every \(5^{th}\) iteration. Recall that we thinned by \(20\) when we
first ran the MCMC algorithm.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{e14Tagm_converged_thinned <-}\StringTok{ }\KeywordTok{mcmc_thin_chains}\NormalTok{(e14Tagm_converged, }\DataTypeTok{freq  =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We initially ran \(6\) chains and, after having made an assessment of
convergence, we decided to discard \(3\) of the chains. We desire to
make inference using samples from all \(3\) chains, since this leads to
better posterior estimates. In their current class structure all the
chains are stored separately, so the following function pools all sample
for all chains together to make a single longer chain with all samplers.
Pooling a mixture of converged and unconverged chains is likely to lead
to poor quality results so should be done with care.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{e14Tagm_converged_pooled <-}\StringTok{ }\KeywordTok{mcmc_pool_chains}\NormalTok{(e14Tagm_converged)}
\NormalTok{e14Tagm_converged_pooled}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Object of class "MCMCParams"
## Method: TAGM.MCMC 
## Number of chains: 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{e14Tagm_converged_pooled[[}\DecValTok{1}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Object of class "MCMCChain"
##  Number of components: 10 
##  Number of proteins: 1663 
##  Number of iterations: 1500
\end{verbatim}

To populate the summary slot of the converged and pooled chain, we can
use the \texttt{tagmMcmcProcess} function. As we can see from the object
below a summary is now available. The information now available in the
summary slot was detailed in the previous section. We note that if there
is more than \(1\) chain in the \texttt{MCMCParams} object then the
chains are automatically pooled to compute the summaries.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{e14Tagm_converged_pooled <-}\StringTok{ }\KeywordTok{tagmMcmcProcess}\NormalTok{(e14Tagm_converged_pooled)}
\NormalTok{e14Tagm_converged_pooled}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Object of class "MCMCParams"
## Method: TAGM.MCMC 
## Number of chains: 1 
## Summary available
\end{verbatim}

To create new feature columns in the \texttt{MSnSet} and append the
summary information, we apply the \texttt{tagmPredict} function. The
\texttt{probJoint} argument indicates whether or not to add
probabilistic information for all organelles for all proteins, rather
than just the information for the most probable organelle. The outlier
probabilities are also returned by default, but users can change this
using the \texttt{probOutlier} argument.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{E14TG2aR <-}\StringTok{ }\KeywordTok{tagmPredict}\NormalTok{(}\DataTypeTok{object =} \NormalTok{E14TG2aR,}
                        \DataTypeTok{params =} \NormalTok{e14Tagm_converged_pooled,}
                        \DataTypeTok{probJoint =} \OtherTok{TRUE}\NormalTok{)}
\KeywordTok{head}\NormalTok{(}\KeywordTok{fData}\NormalTok{(E14TG2aR))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        Uniprot.ID UniprotName
## Q62261     Q62261 SPTB2_MOUSE
## Q9JHU4     Q9JHU4 DYHC1_MOUSE
## Q9QXS1     Q9QXS1  PLEC_MOUSE
## P16546     P16546 SPTA2_MOUSE
## Q69ZN7     Q69ZN7  MYOF_MOUSE
## P30999     P30999 CTND1_MOUSE
##                                     Protein.Description Peptides PSMs
## Q62261 Spectrin beta chain, brain 1 (multiple isoforms)       42   42
## Q9JHU4               Cytoplasmic dynein 1 heavy chain 1       33   33
## Q9QXS1                       Isoform PLEC-1I of Plectin       33   33
## P16546  Spectrin alpha chain, brain (multiple isoforms)       32   32
## Q69ZN7                    Myoferlin (multiple isoforms)       28   28
## P30999              Catenin delta-1 (multiple isoforms)       24   24
##        GOannotation markers.orig         markers   tagm.map.allocation
## Q62261      PLM-SKE      unknown         unknown Endoplasmic reticulum
## Q9JHU4          SKE      unknown         unknown   Nucleus - Chromatin
## Q9QXS1      unknown      unknown         unknown       Plasma membrane
## P16546  PLM-SKE-CYT      unknown         unknown   Nucleus - Chromatin
## Q69ZN7          VES      unknown         unknown       Plasma membrane
## P30999      PLM-NUC          PLM Plasma membrane       Plasma membrane
##        tagm.map.probability tagm.map.outlier  tagm.mcmc.allocation
## Q62261         8.165817e-09     0.9999999857 Endoplasmic reticulum
## Q9JHU4         9.996798e-01     0.0003202255   Nucleus - Chromatin
## Q9QXS1         1.250898e-06     0.9999987491            Proteasome
## P16546         4.226696e-07     0.9999995462 Endoplasmic reticulum
## Q69ZN7         9.994502e-01     0.0001083130       Plasma membrane
## P30999         1.000000e+00     0.0000000000       Plasma membrane
##        tagm.mcmc.probability tagm.mcmc.probability.lowerquantile
## Q62261             0.5765793                        0.0020296117
## Q9JHU4             0.9738206                        0.7594516090
## Q9QXS1             0.4957129                        0.0002886457
## P16546             0.5214374                        0.0014041362
## Q69ZN7             0.9997025                        0.9981794326
## P30999             1.0000000                        1.0000000000
##        tagm.mcmc.probability.upperquantile tagm.mcmc.mean.shannon
## Q62261                           0.9992504            0.201623229
## Q9JHU4                           0.9998822            0.081450206
## Q9QXS1                           0.9947100            0.447665536
## P16546                           0.9946959            0.252833750
## Q69ZN7                           0.9999954            0.002395147
## P30999                           1.0000000            0.000000000
##        tagm.mcmc.outlier tagm.mcmc.joint.40S Ribosome
## Q62261      2.547793e-01                 4.401228e-10
## Q9JHU4      3.335134e-05                 1.936225e-18
## Q9QXS1      6.423799e-01                 2.213861e-07
## P16546      2.119112e-01                 1.576023e-09
## Q69ZN7      7.274103e-06                 3.510523e-22
## P30999      0.000000e+00                 0.000000e+00
##        tagm.mcmc.joint.60S Ribosome tagm.mcmc.joint.Cytosol
## Q62261                 2.778620e-07            2.650861e-12
## Q9JHU4                 1.645727e-21            1.887645e-17
## Q9QXS1                 1.495170e-01            9.062280e-09
## P16546                 3.150122e-06            1.471329e-08
## Q69ZN7                 5.152312e-16            2.063009e-24
## P30999                 0.000000e+00            0.000000e+00
##        tagm.mcmc.joint.Endoplasmic reticulum tagm.mcmc.joint.Lysosome
## Q62261                          5.765793e-01             1.108757e-11
## Q9JHU4                          1.548053e-17             5.577415e-24
## Q9QXS1                          1.768681e-04             1.150706e-04
## P16546                          5.214374e-01             3.687975e-09
## Q69ZN7                          8.397027e-09             2.974966e-04
## P30999                          0.000000e+00             0.000000e+00
##        tagm.mcmc.joint.Mitochondrion tagm.mcmc.joint.Nucleus - Chromatin
## Q62261                  5.020528e-08                        4.231731e-01
## Q9JHU4                  2.835919e-22                        9.738206e-01
## Q9QXS1                  5.832273e-19                        7.920397e-03
## P16546                  4.522032e-08                        4.776913e-01
## Q69ZN7                  6.143974e-39                        4.872032e-21
## P30999                  0.000000e+00                        0.000000e+00
##        tagm.mcmc.joint.Nucleus - Nucleolus tagm.mcmc.joint.Plasma membrane
## Q62261                        1.279255e-05                    1.914808e-11
## Q9JHU4                        2.617943e-02                    3.514851e-29
## Q9QXS1                        1.130580e-05                    3.465462e-01
## P16546                        3.448558e-05                    2.489652e-07
## Q69ZN7                        7.042891e-30                    9.997025e-01
## P30999                        0.000000e+00                    1.000000e+00
##        tagm.mcmc.joint.Proteasome
## Q62261               2.345204e-04
## Q9JHU4               7.841425e-11
## Q9QXS1               4.957129e-01
## P16546               8.333595e-04
## Q69ZN7               1.003778e-10
## P30999               0.000000e+00
\end{verbatim}

\subsubsection*{\texorpdfstring{Aside:
\emph{Priors}}{Aside: Priors}}\label{aside-priors}
\addcontentsline{toc}{subsubsection}{Aside: \emph{Priors}}

Bayesian analysis requires users to specify prior information about the
parameters. This may appear to be a challenging task; however, good
default options are often possible. Should expert information be
available for any of these priors then the users should provide this,
otherwise we have found that the default choices work well in practice.
The priors also provide regularisation and shrinkage to avoid
overfitting. Given enough data the likelihood overwhelms the prior and
the influence of the prior is weak.

We place a normal inverse-Wishart prior on the parameters of the
mutivariate normal mixture components. The normal inverse-Wishart prior
has \(4\) hyperparameters that must be specified. These are: the prior
mean \texttt{mu0} expressing the prior location of each organelle; a
prior shrinkage \texttt{lambda0}, which is a scalar expressing
uncertainty in the prior mean; the prior degrees of freedom
\texttt{nu0}; and a scale prior \texttt{S0} on the covariance. Together,
\texttt{nu0} and \texttt{S0} specify the prior variability on organelle
covariances. The same prior distribution is assumed for the parameters
of all mutivariate normal mixture components.

The default options for these are based on the choice recommended by
(Fraley and Raftery 2005). The prior mean \texttt{mu0} is set to be the
mean of the data. \texttt{lambda0} is set to be \(0.01\) meaning some
uncertainty in the covariance is propagated to the mean, increasing
\texttt{lambda0} increases shrinkage towards the prior. \texttt{nu0} is
set to the number of feature variables plus \(2\), which is the smallest
integer value that ensures a finite covariance matrix. The prior scale
matrix \(S0\) is set to

\begin{equation}
S_0 = \frac{\diag(\frac{1}{n}\sum (X - \bar{X})^2)}{K^{1/D}},
\end{equation}

and represents a diffuse prior on the covariance. Another good choice
which is often used is a constant multiple of the identity matrix. The
prior for the Dirichlet distribution concentration parameters
\texttt{beta0} is set to \(1\) for each organelle. Another reasonable
choice would be the non-informative Jeffery's prior for the Dirichlet
hyperparameter, which sets \texttt{beta0} to \(0.5\) for each organelle.
The prior weight for the outlier detection class is a
\(\mathcal{B}(u, v)\) distribution. The default for \(u = 2\) and the
default for \(v = 10\). This represents the reasonable belief that
\(\frac{u}{u + v} = \frac{1}{6}\) proteins \emph{a priori} might be an
outlier and we believe is unlikely that more than \(50\%\) of proteins
are outliers. Decreasing the value of \(v\), represents more uncertainty
about the number of protein that are outliers.

\subsection{Analysis, visualisation and interpretation of
results}\label{analysis-visualisation-and-interpretation-of-results}

Now that we have a single pooled chain of samples from a converged MCMC
algorithm, we can begin to analyse the results. Preliminary analysis
includes visualising the allocated organelle and localisation
probability of each protein to its most probable organelle, as shown on
figure \ref{fig:mcmcpca}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\KeywordTok{plot2D}\NormalTok{(E14TG2aR, }\DataTypeTok{fcol =} \StringTok{"tagm.mcmc.allocation"}\NormalTok{,}
       \DataTypeTok{cex =} \KeywordTok{fData}\NormalTok{(E14TG2aR)$tagm.mcmc.probability,}
       \DataTypeTok{main =} \StringTok{"TAGM MCMC allocations"}\NormalTok{)}
\KeywordTok{addLegend}\NormalTok{(E14TG2aR, }\DataTypeTok{fcol =} \StringTok{"markers"}\NormalTok{,}
          \DataTypeTok{where =} \StringTok{"topleft"}\NormalTok{, }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{, }\DataTypeTok{cex =} \FloatTok{0.6}\NormalTok{)}

\KeywordTok{plot2D}\NormalTok{(E14TG2aR, }\DataTypeTok{fcol =} \StringTok{"tagm.mcmc.allocation"}\NormalTok{,}
       \DataTypeTok{cex =} \KeywordTok{fData}\NormalTok{(E14TG2aR)$tagm.mcmc.mean.shannon,}
       \DataTypeTok{main =} \StringTok{"Visualising global uncertainty"}\NormalTok{)}
\KeywordTok{addLegend}\NormalTok{(E14TG2aR, }\DataTypeTok{fcol =} \StringTok{"markers"}\NormalTok{,}
          \DataTypeTok{where =} \StringTok{"topleft"}\NormalTok{, }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{, }\DataTypeTok{cex =} \FloatTok{0.6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[htbp]
\centering
\includegraphics{TAGMworkflow_files/figure-latex/mcmcpca-1.pdf}
\caption{\label{fig:mcmcpca}TAGM MCMC allocations. On the left, point size
have been scaled based on allocation probabilities. On the right, the
point size have been scaled based on the global uncertainty using the
mean Shannon entropy.}
\end{figure}

We can visualise other summaries of the data including a Monte-Carlo
averaged Shannon entropy, as shown in figure \ref{fig:mcmcpca} on the
right. This is a measure of uncertainty and proteins with greater
Shannon entropy have more uncertainty in their localisation. We observe
global patterns of uncertainty, particularly in areas where organelle
boundaries overlap. There are also regions of low uncertainty indicating
little doubt about the localisation of these proteins.

We are also interested in the relationship between localisation
probability to the most probable class and the Shannon entropy. Even
though the two quantities are evidently correlated there is still
considerable spread. Thus it is important to base inference not only on
localisation probability but also a measure of uncertainty, for example
the Shannon entropy. Proteins with low Shannon entropy have low
uncertainty in their localisation, whilst those with higher Shannon
entropy have uncertain localisation. Since multi-localised protein have
uncertain localisation to a single subcellular niche, exploring the
Shannon can aid in identifying multi-localised proteins.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cls <-}\StringTok{ }\KeywordTok{getStockcol}\NormalTok{()[}\KeywordTok{as.factor}\NormalTok{(}\KeywordTok{fData}\NormalTok{(E14TG2aR)$tagm.mcmc.allocation)]}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{fData}\NormalTok{(E14TG2aR)$tagm.mcmc.probability,}
     \KeywordTok{fData}\NormalTok{(E14TG2aR)$tagm.mcmc.mean.shannon,}
     \DataTypeTok{col =} \NormalTok{cls, }\DataTypeTok{pch =} \DecValTok{19}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"Localisation probability"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Shannon entropy"}\NormalTok{)}
\KeywordTok{addLegend}\NormalTok{(E14TG2aR, }\DataTypeTok{fcol =} \StringTok{"markers"}\NormalTok{,}
          \DataTypeTok{where =} \StringTok{"topright"}\NormalTok{, }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{, }\DataTypeTok{cex =} \FloatTok{0.6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[htbp]
\centering
\includegraphics{TAGMworkflow_files/figure-latex/probvsentr-1.pdf}
\caption{\label{fig:probvsentr}Shannon entropy and localisation
probability.}
\end{figure}

Aside from global visualisation of the data, we can also interrogate
each individual protein. As illustrated on figure \ref{fig:probdists1},
we can obtain the full posterior distribution of localisation
probabilities for each protein from the
\texttt{e14Tagm\_converged\_pooled} object. We can use the \texttt{plot}
generic on the \texttt{MCMCParams} object to obtain a violin plot of the
localisation distribution. Simply providing the name of the protein in
the second argument produces the plot for that protein. The solute
carrier transporter protein E9QMX3, also referred to as Slc15a1, is most
probably localised to plasma membrane in line with its role as a
transmembrane transporter but also shows some uncertainty, potentially
also localising to other comparments. The first violin plot visualises
this uncertainty. The protein Q3V1Z5 is a supposed constitute of the 40S
ribosome and has poor UniProt annotation with evidence only at the
transcript level. From the plot below is is clear that Q3V1Z5 is a
ribosomal associated protein, but it previous localisation has only been
computational inferred and here we provide experimental evidence of a
ribosomal annotation. Thus, quantifying uncertainty recovers important
additional annotations.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(e14Tagm_converged_pooled, }\StringTok{"E9QMX3"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(e14Tagm_converged_pooled, }\StringTok{"Q3V1Z5"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[htbp]
\centering
\includegraphics{TAGMworkflow_files/figure-latex/probdists1-1.pdf}
\caption{\label{fig:probdists1}Full posterior distribution of localisation
probabilities for individual proteins.}
\end{figure}

\section{Discussion}\label{discussion}

The Bayesian analysis of biological data is of clear interest to many
because of its ability to provide richer information about the
experimental results. A fully Bayesian analysis differs from other
machine learning approaches, since it can quantify the uncertainty in
our inferences. Furthermore, we use a generative model to explicitly
describe the data, which makes inferences more interpretable compared to
the less interpretable outputs of black-box classifiers such as, for
example, support vector machines (SVM).

Bayesian analysis is often characterised by its provision of a
(posterior) probability distribution over the biological parameters of
interest, as opposed to single point estimate of these parameters. In
the case that is presented in this workflow, a Bayesian analysis
``computes'' a posterior probability distribution over the protein
localisation probabilities. These probability distributions can then be
rigorously interrogated for greater biological insight; in addition, it
may allow us to ask additional questions about the data, such as whether
a protein might be multi-localised.

Despite the wealth of information a Bayesian analysis can provide, the
uptake amongst cell biologists is still low. This is because a Bayesian
analysis presents a new set of challenges and little practical guidance
exists regarding how to address these challenges. Bayesian analyses
often rely on computatinally intensive approaches such as Markov-chain
Monte-Carlo (MCMC) and a practical understanding of these algorithms and
the interpretation of their output is a key barrier to their use. A
Bayesian analysis usually consists of three broad steps: (1) Data
pre-processing and algorithmic implementation, (2) assessing algorithmic
convergence and (3) summarising and visualising the results. This
workflow provides a set of tools to simplify these steps and provides
step-by-step guidance in the context of the analysis of spatial
proteomics data.

We have provided a workflow for the Bayesian analysis of spatial
proteomics using the \texttt{pRoloc} and \texttt{MSnbase} software. We
have demonstrated, in a step-by-step fashion, the challenges and
advantages associated with taking a Bayesian approach to data analysis.
We hope this workflow will help spatial proteomics practitioners to
apply our methods and will motivate others to create detailed
documentation for the Bayesian analysis of biological data.

\section{Session information}\label{session-information}

Below, we provide a summary of all packages and versions used to
generate this document.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sessionInfo}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## R version 3.5.2 (2018-12-20)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 17134)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.1252 
## [2] LC_CTYPE=English_United Kingdom.1252   
## [3] LC_MONETARY=English_United Kingdom.1252
## [4] LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.1252    
## 
## attached base packages:
## [1] stats4    parallel  stats     graphics  grDevices utils     datasets 
## [8] methods   base     
## 
## other attached packages:
##  [1] patchwork_0.0.1      pRolocdata_1.20.0    pRoloc_1.23.2       
##  [4] coda_0.19-2          mixtools_1.1.0       BiocParallel_1.16.5 
##  [7] MLInterfaces_1.62.0  cluster_2.0.7-1      annotate_1.60.0     
## [10] XML_3.98-1.17        AnnotationDbi_1.44.0 IRanges_2.16.0      
## [13] MSnbase_2.8.3        ProtGenerics_1.14.0  S4Vectors_0.20.1    
## [16] mzR_2.16.1           Rcpp_1.0.0           Biobase_2.42.0      
## [19] BiocGenerics_0.28.0 
## 
## loaded via a namespace (and not attached):
##   [1] snow_0.4-3            plyr_1.8.4            igraph_1.2.4         
##   [4] lazyeval_0.2.1        splines_3.5.2         ggvis_0.4.4          
##   [7] crosstalk_1.0.0       ggplot2_3.1.0         digest_0.6.18        
##  [10] foreach_1.4.4         htmltools_0.3.6       viridis_0.5.1        
##  [13] gdata_2.18.0          magrittr_1.5          memoise_1.1.0        
##  [16] doParallel_1.0.14     sfsmisc_1.1-3         limma_3.38.3         
##  [19] recipes_0.1.4         gower_0.1.2           rda_1.0.2-2.1        
##  [22] lpSolve_5.6.13        prettyunits_1.0.2     colorspace_1.4-0     
##  [25] blob_1.1.1            xfun_0.5              dplyr_0.8.0.1        
##  [28] crayon_1.3.4          RCurl_1.95-4.11       hexbin_1.27.2        
##  [31] genefilter_1.64.0     impute_1.56.0         survival_2.43-3      
##  [34] iterators_1.0.10      glue_1.3.0            gtable_0.2.0         
##  [37] ipred_0.9-8           zlibbioc_1.28.0       kernlab_0.9-27       
##  [40] prabclus_2.2-7        DEoptimR_1.0-8        scales_1.0.0         
##  [43] vsn_3.50.0            mvtnorm_1.0-8         DBI_1.0.0            
##  [46] viridisLite_0.3.0     xtable_1.8-3          progress_1.2.0       
##  [49] bit_1.1-14            proxy_0.4-22          mclust_5.4.2         
##  [52] preprocessCore_1.44.0 lava_1.6.5            prodlim_2018.04.18   
##  [55] sampling_2.8          htmlwidgets_1.3       httr_1.4.0           
##  [58] threejs_0.3.1         FNN_1.1.3             RColorBrewer_1.1-2   
##  [61] fpc_2.1-11.1          modeltools_0.2-22     pkgconfig_2.0.2      
##  [64] flexmix_2.3-15        nnet_7.3-12           caret_6.0-81         
##  [67] labeling_0.3          tidyselect_0.2.5      rlang_0.3.1          
##  [70] reshape2_1.4.3        later_0.8.0           munsell_0.5.0        
##  [73] mlbench_2.1-1         tools_3.5.2           LaplacesDemon_16.1.1 
##  [76] generics_0.0.2        RSQLite_2.1.1         pls_2.7-0            
##  [79] evaluate_0.13         stringr_1.4.0         mzID_1.20.1          
##  [82] yaml_2.2.0            ModelMetrics_1.2.2    knitr_1.21           
##  [85] bit64_0.9-7           robustbase_0.93-3     randomForest_4.6-14  
##  [88] purrr_0.3.0           dendextend_1.9.0      ncdf4_1.16           
##  [91] nlme_3.1-137          whisker_0.3-2         mime_0.6             
##  [94] biomaRt_2.38.0        compiler_3.5.2        rstudioapi_0.9.0     
##  [97] e1071_1.7-0.1         affyio_1.52.0         tibble_2.0.1         
## [100] stringi_1.3.1         highr_0.7             lattice_0.20-38      
## [103] trimcluster_0.1-2.1   Matrix_1.2-15         gbm_2.1.5            
## [106] pillar_1.3.1          BiocManager_1.30.4    MALDIquant_1.18      
## [109] data.table_1.12.0     bitops_1.0-6          httpuv_1.4.5.1       
## [112] R6_2.4.0              pcaMethods_1.74.0     affy_1.60.0          
## [115] hwriter_1.3.2         bookdown_0.9          promises_1.0.1       
## [118] gridExtra_2.3         codetools_0.2-15      MASS_7.3-51.1        
## [121] gtools_3.8.1          assertthat_0.2.0      withr_2.1.2          
## [124] diptest_0.75-7        hms_0.4.2             timeDate_3043.102    
## [127] grid_3.5.2            rpart_4.1-13          class_7.3-14         
## [130] rmarkdown_1.11        segmented_0.5-3.0     lubridate_1.7.4      
## [133] shiny_1.2.0           base64enc_0.1-3
\end{verbatim}

The source of this document, including the code necessary to reproduce
the analyses and figures is available in a public manuscript repository
on GitHub (Crook and Gatto 2019).

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\hypertarget{ref-Beltran:2016}{}
Beltran, Pierre M Jean, Rommel A Mathias, and Ileana M Cristea. 2016.
``A Portrait of the Human Organelle Proteome in Space and Time During
Cytomegalovirus Infection.'' \emph{Cell Systems} 3 (4). Elsevier:
361--73.

\hypertarget{ref-Breckels:2013}{}
Breckels, Lisa M, Laurent Gatto, Andy Christoforou, Arnoud J Groen,
Kathryn S Lilley, and Matthew WB Trotter. 2013. ``The Effect of
Organelle Discovery Upon Sub-Cellular Protein Localisation.''
\emph{Journal of Proteomics} 88. Elsevier: 129--40.

\hypertarget{ref-Breckels:2016}{}
Breckels, Lisa M, Sean B Holden, David Wojnar, Claire M Mulvey, Andy
Christoforou, Arnoud Groen, Matthew WB Trotter, Oliver Kohlbacher,
Kathryn S Lilley, and Laurent Gatto. 2016. ``Learning from Heterogeneous
Data Sources: An Application in Spatial Proteomics.'' \emph{PLoS
Computational Biology} 12 (5). Public Library of Science: e1004920.

\hypertarget{ref-Breckels:2016b}{}
Breckels, Lisa M, Claire M Mulvey, Kathryn S Lilley, and Laurent Gatto.
2016. ``A Bioconductor Workflow for Processing and Analysing Spatial
Proteomics Data.'' \emph{F1000Research} 5.

\hypertarget{ref-Brooks:1998}{}
Brooks, Stephen P, and Andrew Gelman. 1998. ``General Methods for
Monitoring Convergence of Iterative Simulations.'' \emph{Journal of
Computational and Graphical Statistics} 7 (4). Taylor \& Francis:
434--55.

\hypertarget{ref-hyper}{}
Christoforou, Andy, Claire M Mulvey, Lisa M Breckels, Aikaterini
Geladaki, Tracey Hurrell, Penelope C Hayward, Thomas Naake, et al. 2016.
``A Draft Map of the Mouse Pluripotent Stem Cell Spatial Proteome.''
\emph{Nature Communications} 7. Nature Publishing Group: 9992.

\hypertarget{ref-Cody:2013}{}
Cody, Neal AL, Carole Iampietro, and Eric LÃ©cuyer. 2013. ``The Many
Functions of MRNA Localization During Normal Development and Disease:
From Pillar to Post.'' \emph{Wiley Interdisciplinary Reviews:
Developmental Biology} 2 (6). Wiley Online Library: 781--96.

\hypertarget{ref-Crook:2018}{}
Crook, Oliver M, Claire M Mulvey, Paul D W Kirk, Kathryn S Lilley, and
Laurent Gatto. 2018. ``A Bayesian Mixture Modelling Approach for Spatial
Proteomics.'' \emph{PLoS Comput. Biol.} 14 (11): e1006516.

\hypertarget{ref-ghrepo}{}
Crook, OM, and L Gatto. 2019. ``A Bioconductor Workflow for the Bayesian
Analysis of Spatial Proteomics.'' \emph{GitHub Repository}.
\url{https://github.com/ococrook/TAGMworkflow}; GitHub.

\hypertarget{ref-Davies:2018}{}
Davies, Alexandra K, Daniel N Itzhak, James R Edgar, Tara L Archuleta,
Jennifer Hirst, Lauren P Jackson, Margaret S Robinson, and Georg HH
Borner. 2018. ``AP-4 Vesicles Contribute to Spatial Control of Autophagy
via Rusc-Dependent Peripheral Delivery of Atg9a.'' \emph{Nature
Communications} 9. Nature Publishing Group: 3958.

\hypertarget{ref-De:2011}{}
De Matteis, Maria Antonietta, and Alberto Luini. 2011. ``Mendelian
Disorders of Membrane Trafficking.'' \emph{New England Journal of
Medicine} 365 (10). Mass Medical Soc: 927--38.

\hypertarget{ref-EM:1977}{}
Dempster, Arthur P, Nan M Laird, and Donald B Rubin. 1977. ``Maximum
Likelihood from Incomplete Data via the Em Algorithm.'' \emph{Journal of
the Royal Statistical Society. Series B (Methodological)}. JSTOR, 1--38.

\hypertarget{ref-Dunkley:2006}{}
Dunkley, Tom PJ, Svenja Hester, Ian P Shadforth, John Runions, Thilo
Weimar, Sally L Hanton, Julian L Griffin, et al. 2006. ``Mapping the
Arabidopsis Organelle Proteome.'' \emph{Proceedings of the National
Academy of Sciences} 103 (17). National Acad Sciences: 6518--23.

\hypertarget{ref-Dunkley:2004}{}
Dunkley, Tom PJ, Rod Watson, Julian L Griffin, Paul Dupree, and Kathryn
S Lilley. 2004. ``Localization of Organelle Proteins by Isotope Tagging
(Lopit).'' \emph{Molecular \& Cellular Proteomics} 3 (11). ASBMB:
1128--34.

\hypertarget{ref-Foster:2006}{}
Foster, Leonard J, Carmen L de Hoog, Yanling Zhang, Yong Zhang, Xiaohui
Xie, Vamsi K Mootha, and Matthias Mann. 2006. ``A Mammalian Organelle
Map by Protein Correlation Profiling.'' \emph{Cell} 125 (1). Elsevier:
187--99.

\hypertarget{ref-Fraley:2005}{}
Fraley, Chris, and Adrian E Raftery. 2005. ``Bayesian Regularization for
Normal Mixture Estimation and Model-Based Clustering.'' \emph{Techincal
Report}. Washington Univ Seattle Dept of Statistics.

\hypertarget{ref-pRoloc:2014}{}
Gatto, Laurent, Lisa M. Breckels, Samuel Wieczorek, Thomas Burger, and
Kathryn S. Lilley. 2014. ``Mass-Spectrometry Based Spatial Proteomics
Data Analysis Using PRoloc and PRolocdata.'' \emph{Bioinformatics}.

\hypertarget{ref-DC:2018}{}
Geladaki, Aikaterini, Nina Kocevar Britovsek, Lisa M Breckels, Tom Sand
Owen L Vennard Smith, Claire M Mulvey, Oliver M Crook, Laurent Gatto,
and Kathryn S Lilley. 2019. ``Combining Lopit with Differential
Ultracentrifugation for High-Resolution Spatial Proteomics.''
\emph{Nature Communications} 10. Nature Publishing Group: 331.

\hypertarget{ref-Gelman:1995}{}
Gelman, A., J. B. Carlin, H. S. Stern, and D. B. Rubin. 1995.
\emph{Bayesian Data Analysis}. London: Chapman \& Hall.

\hypertarget{ref-Gelman:1992}{}
Gelman, Andrew, and Donald B Rubin. 1992. ``Inference from Iterative
Simulation Using Multiple Sequences.'' \emph{Statistical Science}.
JSTOR, 457--72.

\hypertarget{ref-Geweke:1992}{}
Geweke, John. 1992. ``Evaluating the Accuracy of Sampling-Based
Approaches to the Calculation of Posterior Moments.'' \emph{BAYESIAN
STATISTICS}. Citeseer.

\hypertarget{ref-Gibson:2009}{}
Gibson, Toby J. 2009. ``Cell Regulation: Determined to Signal Discrete
Cooperation.'' \emph{Trends in Biochemical Sciences} 34 (10). Elsevier:
471--82.

\hypertarget{ref-Gilks:1995}{}
Gilks, Walter R, Sylvia Richardson, and David Spiegelhalter. 1995.
\emph{Markov Chain Monte Carlo in Practice}. Chapman; Hall/CRC.

\hypertarget{ref-Hall:2009}{}
Hall, Stephanie L, Svenja Hester, Julian L Griffin, Kathryn S Lilley,
and Antony P Jackson. 2009. ``The Organelle Proteome of the Dt40
Lymphocyte Cell Line.'' \emph{Molecular \& Cellular Proteomics} 8 (6).
ASBMB: 1295--1305.

\hypertarget{ref-Hirst:2018}{}
Hirst, Jennifer, Daniel N Itzhak, Robin Antrobus, Georg HH Borner, and
Margaret S Robinson. 2018. ``Role of the Ap-5 Adaptor Protein Complex in
Late Endosome-to-Golgi Retrieval.'' \emph{PLoS Biology} 16 (1). Public
Library of Science: e2004411.

\hypertarget{ref-Itzhak:2017}{}
Itzhak, Daniel N, Colin Davies, Stefka Tyanova, Archana Mishra, James
Williamson, Robin Antrobus, JÃ¼rgen Cox, Michael P Weekes, and Georg HH
Borner. 2017. ``A Mass Spectrometry-Based Approach for Mapping Protein
Subcellular Localization Reveals the Spatial Proteome of Mouse Primary
Neurons.'' \emph{Cell Reports} 20 (11). Elsevier: 2706--18.

\hypertarget{ref-Itzhak:2016}{}
Itzhak, Daniel N, Stefka Tyanova, JÃ¼rgen Cox, and Georg HH Borner. 2016.
``Global, Quantitative and Dynamic Mapping of Protein Subcellular
Localization.'' \emph{Elife} 5. eLife Sciences Publications Limited:
e16950.

\hypertarget{ref-Jadot:2017}{}
Jadot, Michel, Marielle Boonen, Jaqueline Thirion, Nan Wang, Jinchuan
Xing, Caifeng Zhao, Abla Tannous, et al. 2017. ``Accounting for Protein
Subcellular Localization: A Compartmental Map of the Rat Liver
Proteome.'' \emph{Molecular \& Cellular Proteomics} 16 (2). ASBMB:
194--212.

\hypertarget{ref-Jeffery:2009}{}
Jeffery, Constance J. 2009. ``Moonlighting Proteins - an Update.''
\emph{Molecular BioSystems} 5 (4). Royal Society of Chemistry: 345--50.

\hypertarget{ref-Kau:2004}{}
Kau, Tweeny R, Jeffrey C Way, and Pamela A Silver. 2004. ``Nuclear
Transport and Cancer: From Mechanism to Intervention.'' \emph{Nature
Reviews Cancer} 4 (2). Nature Publishing Group: 106--17.

\hypertarget{ref-Latorre:2005}{}
Latorre, Isabel J, Michael H Roh, Kristopher K Frese, Robert S Weiss,
Ben Margolis, and Ronald T Javier. 2005. ``Viral Oncoprotein-Induced
Mislocalization of Select Pdz Proteins Disrupts Tight Junctions and
Causes Polarity Defects in Epithelial Cells.'' \emph{Journal of Cell
Science} 118 (18). The Company of Biologists Ltd: 4283--93.

\hypertarget{ref-Laurila:2009}{}
Laurila, Kirsti, and Mauno Vihinen. 2009. ``Prediction of
Disease-Related Mutations Affecting Protein Localization.'' \emph{BMC
Genomics} 10 (1). BioMed Central: 122.

\hypertarget{ref-Luheshi:2008}{}
Luheshi, Leila M, Damian C Crowther, and Christopher M Dobson. 2008.
``Protein Misfolding and Disease: From the Test Tube to the Organism.''
\emph{Current Opinion in Chemical Biology} 12 (1). Elsevier: 25--31.

\hypertarget{ref-Mendes:2017}{}
Mendes, Marta, Alberto PelÃ¡ez-GarcÃ­a, MarÃ­a LÃ³pez-Lucendo, RubÃ©n A.
BartolomÃ©, Eva CalviÃ±o, Rodrigo Barderas, and J. Ignacio Casal. 2017.
``Mapping the Spatial Proteome of Metastatic Cells in Colorectal
Cancer.'' \emph{Proteomics} 17 (19).
doi:\href{https://doi.org/10.1002/pmic.201700094}{10.1002/pmic.201700094}.

\hypertarget{ref-Mulvey:2017}{}
Mulvey, Claire M, Lisa M Breckels, Aikaterini Geladaki, Nina KoÄevar
BritovÅ¡ek, Daniel JH Nightingale, Andy Christoforou, Mohamed Elzek,
Michael J Deery, Laurent Gatto, and Kathryn S Lilley. 2017. ``Using
hyperLOPIT to Perform High-Resolution Mapping of the Spatial Proteome.''
\emph{Nature Protocols} 12 (6). Nature Research: 1110--35.

\hypertarget{ref-Nightingale:2019}{}
Nightingale, Daniel JH, Aikaterini Geladaki, Lisa M Breckels, Stephen G
Oliver, and Kathryn S Lilley. 2019. ``The Subcellular Organisation of
Saccharomyces Cerevisiae.'' \emph{Current Opinion in Chemical Biology}
48. Elsevier: 86--95.

\hypertarget{ref-Olkkonen:2006}{}
Olkkonen, Vesa M, and Elina Ikonen. 2006. ``When Intracellular Logistics
Fails-Genetic Defects in Membrane Trafficking.'' \emph{Journal of Cell
Science} 119 (24). The Company of Biologists Ltd: 5031--45.

\hypertarget{ref-Orre:2019}{}
Orre, Lukas Minus, Mattias Vesterlund, Yanbo Pan, Taner Arslan, Yafeng
Zhu, Alejandro Fernandez Woodbridge, Oliver Frings, Erik Fredlund, and
Janne LehtiÃ¶. 2019. ``SubCellBarCode: Proteome-Wide Mapping of Protein
Localization and Relocalization.'' \emph{Molecular Cell} 73 (1):
166--182.e7.
doi:\href{https://doi.org/https://doi.org/10.1016/j.molcel.2018.11.035}{https://doi.org/10.1016/j.molcel.2018.11.035}.

\hypertarget{ref-coda}{}
Plummer, Martyn, Nicky Best, Kate Cowles, and Karen Vines. 2006. ``CODA:
Convergence Diagnosis and Output Analysis for Mcmc.'' \emph{R News} 6
(1): 7--11. \url{https://journal.r-project.org/archive/}.

\hypertarget{ref-Roberts:1994}{}
Roberts, Gareth O, and Adrian FM Smith. 1994. ``Simple Conditions for
the Convergence of the Gibbs Sampler and Metropolis-Hastings
Algorithms.'' \emph{Stochastic Processes and Their Applications} 49 (2).
Elsevier: 207--16.

\hypertarget{ref-Rodriguez:2004}{}
Rodriguez, JosÃ© Antonio, Wendy WY Au, and Beric R Henderson. 2004.
``Cytoplasmic Mislocalization of Brca1 Caused by Cancer-Associated
Mutations in the Brct Domain.'' \emph{Experimental Cell Research} 293
(1). Elsevier: 14--21.

\hypertarget{ref-Shin:2013}{}
Shin, Soo J, Jeffrey A Smith, GÃ¼nther A Rezniczek, Sheng Pan, Ru Chen,
Teresa A Brentnall, Gerhard Wiche, and Kimberly A Kelly. 2013.
``Unexpected Gain of Function for the Scaffolding Protein Plectin Due to
Mislocalization in Pancreatic Cancer.'' \emph{Proceedings of the
National Academy of Sciences} 110 (48). National Acad Sciences:
19414--9.

\hypertarget{ref-Siljee:2018}{}
Siljee, J E, Y Wang, A A Bernard, B A Ersoy, S Zhang, A Marley, M Von
Zastrow, J F Reiter, and C Vaisse. 2018. ``Subcellular Localization of
MC4R with ADCY3 at Neuronal Primary Cilia Underlies a Common Pathway for
Genetic Predisposition to Obesity.'' \emph{Nat Genet}, January.
doi:\href{https://doi.org/10.1038/s41588-017-0020-9}{10.1038/s41588-017-0020-9}.

\hypertarget{ref-Smith:1993}{}
Smith, Adrian FM, and Gareth O Roberts. 1993. ``Bayesian Computation via
the Gibbs Sampler and Related Markov Chain Monte Carlo Methods.''
\emph{Journal of the Royal Statistical Society. Series B
(Methodological)}. JSTOR, 3--23.

\hypertarget{ref-Tan:2009}{}
Tan, Denise JL, Heidi Dvinge, Andrew Christoforou, Paul Bertone, Alfonso
Martinez Arias, and Kathryn S Lilley. 2009. ``Mapping Organelle Proteins
and Protein Complexes in Drosophila Melanogaster.'' \emph{Journal of
Proteome Research} 8 (6). ACS Publications: 2667--78.

\hypertarget{ref-Thul:2017}{}
Thul, Peter J, Lovisa Ãkesson, Mikaela Wiking, Diana Mahdessian,
Aikaterini Geladaki, Hammou Ait Blal, Tove Alm, et al. 2017. ``A
Subcellular Map of the Human Proteome.'' \emph{Science} 356 (6340).
American Association for the Advancement of Science: eaal3321.


\end{document}
